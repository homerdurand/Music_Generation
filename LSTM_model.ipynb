{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools as it\n",
    "import random as rd\n",
    "\n",
    "#System\n",
    "import os \n",
    "from glob import glob\n",
    "\n",
    "import re\n",
    "\n",
    "#Music Analysis\n",
    "import music21 as m21\n",
    "\n",
    "#Encodage\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "#LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, LSTM, Dropout, Flatten\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "\n",
    "m21.environment.set('musescoreDirectPNGPath', '/Applications/MuseScore 3.app/contents')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentenceEncoding(sentence, order = \"character\", sepTime = False) :\n",
    "    #retourne encodage one hot de la phrase, integer encode de la phrase et la taille du vocabulaire\n",
    "    #Ordre = chara ou word\n",
    "    if order == \"character\" :\n",
    "        if sepTime :\n",
    "            sentence = sentence.replace(\"°\", \"_\")\n",
    "        token = sentence.split(\"_\")\n",
    "    elif order == \"word\" :\n",
    "        token = sentence.split(\" \")\n",
    "        #A finir\n",
    "    \n",
    "    # integer encode\n",
    "    label_encoder = LabelEncoder()\n",
    "    integer_encoded = label_encoder.fit_transform(token)\n",
    "    # binary encode\n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "    onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "    \n",
    "    vocabLength = int(max(integer_encoded))\n",
    "    return onehot_encoded, integer_encoded, token, vocabLength"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding and midi creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_float(frac_str):\n",
    "    try:\n",
    "        return float(frac_str)\n",
    "    except ValueError:\n",
    "        num, denom = frac_str.split('/')\n",
    "        try:\n",
    "            leading, num = num.split(' ')\n",
    "            whole = float(leading)\n",
    "        except ValueError:\n",
    "            whole = 0\n",
    "        frac = float(num) / float(denom)\n",
    "        return whole - frac if whole < 0 else whole + frac\n",
    "\n",
    "def word2chord(word, duration = True) :\n",
    "    if \"Start\" in word :\n",
    "        word = word[6:-1]\n",
    "    elif \"End\" in word : \n",
    "        if len(word)==3 :\n",
    "            return False\n",
    "        word = word[1:-3]\n",
    "    else :\n",
    "        word = word[1:-1]\n",
    "        \n",
    "    chordTemp = []\n",
    "    for letter in word.split(\"_\") :\n",
    "        noteTemp, durationTemp = letter.split(\"°\")\n",
    "        duration = m21.duration.Duration(convert_to_float(durationTemp))\n",
    "        note = m21.note.Note(noteTemp, duration = duration)\n",
    "        chordTemp.append(note)\n",
    "    chord = m21.chord.Chord(chordTemp)\n",
    "    return chord\n",
    "\n",
    "def token2midi(token):\n",
    "    s = m21.stream.Stream()\n",
    "    flag = False\n",
    "    i=0\n",
    "    while i <len(token) :\n",
    "        chordTemp = []\n",
    "        while token[i] != \" \" and not(\"Start\" in token[i]) and not(\"End\" in token[i]) and i <len(token)-1 :\n",
    "            word = token[i]\n",
    "            flag = True\n",
    "            noteTemp, durationTemp = word.split(\"°\")\n",
    "            duration = m21.duration.Duration(convert_to_float(durationTemp))\n",
    "            note = m21.note.Note(noteTemp, duration = duration)\n",
    "            chordTemp.append(note)\n",
    "            i+=1\n",
    "        if flag :\n",
    "            chord = m21.chord.Chord(chordTemp)\n",
    "            s.append(chord)\n",
    "            flag = False\n",
    "        i+=1\n",
    "            \n",
    "    return s\n",
    "\n",
    "def sentence2midi(sentence) :\n",
    "    s = m21.stream.Stream()\n",
    "    for wordTemp in sentence.split(\" \")[5:-5] :\n",
    "        thisChord = word2chord(wordTemp)\n",
    "        if thisChord :#On vérifie que l'accord n'est pas vide\n",
    "            s.append(thisChord)\n",
    "    return s\n",
    "\n",
    "def token2sentence(token, order=\"character\", sepTime = False) :\n",
    "    sentence=\"\"\n",
    "    if order == \"character\" :\n",
    "        if sepTime :\n",
    "            \n",
    "                if \"Start\" in word :\n",
    "                    sentence+=word+\"_\"\n",
    "                elif \".\" in word : # C'est une durée\n",
    "                    sentence+=\"°\"+word+\"_\"\n",
    "                elif \" \" in word:# C'est un espace\n",
    "                    sentence+=word+\"_\"\n",
    "                else :\n",
    "                    sentence+=word\n",
    "        else :\n",
    "            for word in token :\n",
    "                sentence+=word+\"_\"\n",
    "        return sentence[:-1]\n",
    "    elif order == \"word\" :\n",
    "        for word in token :\n",
    "            sentence+=word+\" \"\n",
    "    return sentence[:-1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Artist</th>\n",
       "      <th>File</th>\n",
       "      <th>Piece</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Test2</td>\n",
       "      <td>keithkoln.mid</td>\n",
       "      <td>_B-3°0.25_ _F4°0.5_ _E-4°5.25_ _B-3°8.25_ _C4°...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Artist           File                                              Piece\n",
       "0  Test2  keithkoln.mid  _B-3°0.25_ _F4°0.5_ _E-4°5.25_ _B-3°8.25_ _C4°..."
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "musicDB = pd.read_pickle(\"keithKoln.pkl\")\n",
    "musicDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'_B-3°0.25_ _F4°0.5_ _E-4°5.25_ _B-3°8.25_ _C4°8.5_ _F3°2.0_F2°0.5_ _C3°7.5_ _F3°2.0_ _F3°2.0_ _F3°2.0_ _B-3°0.25_F3°2.0_ _F4°0.5_ _E-4°0.25_ _B-3°0.25_ _F4°0.25_C4°3.0_ _G4°0.25_ _G4°0.0_A-4°0.5_F3°2.0_ _G4°0.25_ _F4°0.5_ _E-4°5.5_ _G2°2.25_ _G3°0.25_C2°1.0_D3°1.0_C2°1.0_D3°1.0_ _C4°0.5_ _B-3°0.25_ _F3°0.25_ _G3°4.0_ _F3°2.0_C3°1.5_ _G2°1.25_ _C2°3.5_D3°2.0_F3°0.0_ _C4°0.25_G2°2.5_ _C4°0.25_ _E-4°0.5_D3°0.0_ _E-4°0.25_ _E-4°0.25_ _C4°0.25_ _C4°0.25_ _C4°0.5_ _G3°0.0_E-2°0.25_ _A-3°0.0_B-3°0.5_C3°1.0_B-1°1.0_E-3°1.0_C3°1.0_B-1°1.0_E-3°1.0_C3°1.0_B-1°1.0_E-3°1.0_ _B-3°0.0_ _G3°0.5_ _G3°1.0_B-3°1.0_G3°1.0_B-3°1.0_ _E-3°0.5_B-3°0.0_ _E-3°0.0_B-3°2.0_C3°1.0_E-3°1.0_C3°1.0_E-3°1.0_ _E-3°0.75_ _E-2°0.25_ _E-3°2.5_A-1°1.5_C3°2.0_ _E-2°0.5_ _C3°0.5_ _E-3°0.25_ _E-3°0.25_ _F3°0.25_ _G3°0.25_F2°0.25_ _B-3°0.5_F3°3.0_B-1°1.0_D3°1.0_B-1°1.0_D3°1.0_ _D4°0.0_ _E-4°0.25_ _D4°0.5_ _B-3°0.25_ _C4°0.25_ _B-3°3.0_D3°2.0_ _F3°0.5_ _F3°0.5_ _F3°0.5_B-1°1.0_D3°2.0_ _F3°0.5_ _F3°0.5_ _B-1°0.25_ _B-3°1.0_F3°1.0_B-3°1.0_F3°1.0_C2°0.25_ _D2°0.25_ _A-3°0.75_F3°0.5_E-2°1.0_ _F3°0.25_ _G3°1.0_E-3°1.0_G3°1.0_E-3°1.0_ _D3°1.0_F3°1.0_D3°1.0_F3°1.0_D2°0.25_ _C2°0.25_ _B-1°0.25_ _C3°0.25_G1°0.25_ _B-3°1.0_F3°1.0_B-3°1.0_F3°1.0_C3°1.0_C2°1.0_C3°1.0_C2°1.0_ _E-4°0.25_ _F4°0.25_ _E-4°0.5_ _E-4°0.0_D4°0.25_ _B-3°0.5_ _C4°4.0_D3°2.0_ _F3°0.5_ _F3°0.5_ _F3°0.5_D3°1.0_C3°1.0_D3°1.0_C3°1.0_ _D3°0.25_ _F3°0.5_D3°1.0_C3°1.0_D3°1.0_C3°1.0_ _D3°0.25_ _F3°0.5_B-2°1.0_C3°1.0_B-2°1.0_C3°1.0_ _F3°0.5_ _F3°0.5_C3°1.0_B-2°1.0_C3°1.0_B-2°1.0_ _C3°0.25_ _F3°0.5_B-2°1.0_C3°1.0_B-2°1.0_C3°1.0_ _C3°0.25_ _D4°0.25_G2°1.0_B-2°1.0_G2°1.0_B-2°1.0_ _E-4°0.25_ _F4°0.25_ _F4°0.0_ _G4°0.25_ _B-2°1.0_D2°1.0_G1°1.0_B-2°1.0_D2°1.0_G1°1.0_B-2°1.0_D2°1.0_G1°1.0_ _F4°0.25_ _D4°0.0_ _C4°0.25_ _B-3°0.25_ _G3°1.0_B-2°1.5_ _F3°1.0_B-3°1.0_G3°1.0_F3°1.0_B-3°1.0_G3°1.0_F3°1.0_B-3°1.0_G3°1.0_ _B-2°0.25_ _B-3°1.0_F3°1.0_B-3°1.0_F3°1.0_G3°0.0_B-2°2.0_D2°4.0_G1°1.75_ _G1°2.0_ _G3°1.0_B-3°1.0_F3°1.0_G3°1.0_B-3°1.0_F3°1.0_G3°1.0_B-3°1.0_F3°1.0_B-2°0.75_ _B-3°0.5_ _C4°1/3_ _C4°0.0_C2°0.25_ _C#4°0.0_D4°1/3_F1°1.0_A2°1.0_F1°1.0_A2°1.0_ _C4°0.25_ _C4°0.25_ _C4°0.25_ _F3°0.5_ _G3°0.0_A3°1.0_F2°1.75_C3°0.75_ _C3°1.25_ _B-3°0.25_ _C4°0.25_ _B-3°2.5_ _E-2°0.25_ _G3°0.5_A-1°1.0_C3°1.0_A-1°1.0_C3°1.0_ _G3°0.25_ _G3°0.25_ _G3°0.25_ _C3°0.25_ _E-3°1.0_A-2°1.75_ _F3°1.0_D3°1.0_F3°1.0_D3°1.0_ _D3°0.25_F3°0.5_ _C3°0.25_ _B-2°0.25_F2°0.25_ _D3°0.75_B-3°7.75_F3°2.0_B-2°6.0_B-1°1.0_ _D3°0.25_ _D3°0.25_B-1°0.25_ _E-3°0.25_C2°0.25_ _D3°0.25_B-1°0.25_ _F3°5.0_D2°6.0_ _B-4°1.0_C4°1.0_F4°1.0_B-4°1.0_C4°1.0_F4°1.0_B-4°1.0_C4°1.0_F4°1.0_G1°1.0_B-2°1.0_G1°1.0_B-2°1.0_ _B-4°1.0_C4°1.0_B-4°1.0_C4°1.0_F4°0.5_C2°1.0_E-3°1.0_C2°1.0_E-3°1.0_ _F4°0.25_ _F4°0.5_ _F4°0.25_ _D4°0.25_ _C4°4.0_G3°1.0_D3°1.0_E-3°1.0_G3°1.0_D3°1.0_E-3°1.0_G3°1.0_D3°1.0_E-3°1.0_ _G3°1.0_D3°1.0_E-3°1.0_G3°1.0_D3°1.0_E-3°1.0_G3°1.0_D3°1.0_E-3°1.0_ _D3°1.0_E-3°1.0_G3°1.0_D3°1.0_E-3°1.0_G3°1.0_D3°1.0_E-3°1.0_G3°1.0_ _D3°1.0_G3°1.0_E-3°1.0_D3°1.0_G3°1.0_E-3°1.0_D3°1.0_G3°1.0_E-3°1.0_ _B-2°1.0_E-3°1.0_D3°1.0_B-2°1.0_E-3°1.0_D3°1.0_B-2°1.0_E-3°1.0_D3°1.0_ _C2°1.0_G2°1.0_C2°1.0_G2°1.0_ _C2°1.0_G2°1.0_C2°1.0_G2°1.0_ _E-4°0.25_F2°1.0_C3°1.0_F2°1.0_C3°1.0_ _E-4°0.25_ _E-4°0.25_ _E-4°0.25_ _F4°0.25_F2°1.0_C3°1.0_F2°1.0_C3°1.0_ _E-4°0.25_D3°1.0_G2°1.0_D3°1.0_G2°1.0_ _D4°0.25_ _C4°0.25_B-2°1.0_F2°1.0_B-2°1.0_F2°1.0_ _D4°0.0_E-4°0.5_B-1°2.0_ _D4°0.25_ _D4°0.25_ _D4°0.25_ _C4°0.25_ _B-3°1.5_F2°1.0_B-1°1.0_F2°1.0_B-1°1.0_ _B-2°1.0_E-2°1.0_B-2°1.0_E-2°1.0_ _D3°1.0_F2°1.0_D3°1.0_F2°1.0_ _F2°1.0_D3°1.0_F2°1.0_D3°1.0_ _B-3°3.0_ _E-2°1.0_B-2°1.0_E-2°1.0_B-2°1.0_ _B-1°1.0_F2°1.0_B-1°1.0_F2°1.0_ _B-1°1.0_F2°1.0_B-1°1.0_F2°1.0_ _F2°1.0_B-1°1.0_F2°1.0_B-1°1.0_ _F2°1.0_B-1°1.0_F2°1.0_B-1°1.0_ _B-2°1.0_E-2°1.0_B-2°1.0_E-2°1.0_ _E-4°0.25_F2°1.0_D3°1.0_F2°1.0_D3°1.0_ _D4°0.25_ _E-4°0.25_ _D4°0.25_E-2°1.0_C3°1.0_E-2°1.0_C3°1.0_ _C4°0.25_ _B-3°0.0_E-2°1.0_C3°1.0_E-2°1.0_C3°1.0_ _B-3°1.0_B3°1.0_B-3°1.0_B3°1.0_C4°0.5_G1°1.0_D2°1.0_G1°1.0_D2°1.0_ _B-3°0.25_ _B-3°0.25_ _B-3°0.25_ _G3°0.25_ _D2°1.0_G1°1.0_D2°1.0_G1°1.0_ _G3°3.5_ _G1°0.5_ _A2°1.0_C2°1.0_A2°1.0_C2°1.0_ _B-2°1.0_D2°1.0_B-2°1.0_D2°1.0_ _E-2°1.0_C3°1.0_E-2°1.0_C3°1.0_ _D2°1.0_B-2°1.0_D2°1.0_B-2°1.0_ _A2°1.0_C2°1.0_A2°1.0_C2°1.0_ _G2°1.0_B-1°1.0_G2°1.0_B-1°1.0_ _G3°0.25_ _B-3°0.25_G1°1.0_D2°1.0_G1°1.0_D2°1.0_ _C4°0.25_ _G3°0.0_ _A3°0.5_ _F1°1.0_C2°1.0_F1°1.0_C2°1.0_ _G3°0.25_ _F3°0.25_ _B-2°0.25_F2°0.25_ _E-3°0.5_B-3°2.0_B-1°1.0_F2°1.0_B-1°1.0_F2°1.0_ _D3°0.25_ _D3°0.25_ _B-2°0.25_ _C3°1.0_E-3°1.0_C3°1.0_E-3°1.0_B-1°1.0_E-2°1.0_G2°1.0_B-1°1.0_E-2°1.0_G2°1.0_B-1°1.0_E-2°1.0_G2°1.0_ _B-2°1.75_F3°1.0_D3°1.0_F3°1.0_D3°1.0_B-1°1.0_F2°1.0_B-1°1.0_F2°1.0_ _B-2°0.25_ _G3°1.0_E-3°1.0_G3°1.0_E-3°1.0_G2°1.0_B-1°1.0_G2°1.0_B-1°1.0_ _F3°0.25_ _D3°1.0_F3°1.0_D3°1.0_F3°1.0_B-1°1.0_F2°1.0_B-1°1.0_F2°1.0_ _B-2°0.25_ _E-3°1.0_G3°1.0_E-3°1.0_G3°1.0_B-1°1.0_G2°1.0_B-1°1.0_G2°1.0_ _D3°1.0_F3°1.0_D3°1.0_F3°1.0_ _D3°1.0_F3°1.0_D3°1.0_F3°1.0_B-1°1.0_F2°1.0_B-1°1.0_F2°1.0_ _B-2°0.25_ _C4°4.0_A3°1.0_F3°1.0_A3°1.0_F3°1.0_C2°1.0_A2°1.0_C2°1.0_A2°1.0_ _G3°1.0_E3°1.0_G3°1.0_E3°1.0_G2°0.25_ _G3°1.0_E3°1.0_G3°1.0_E3°1.0_G2°0.25_ _E3°1.0_C3°1.0_E3°1.0_C3°1.0_ _D3°1.0_F3°1.0_D3°1.0_F3°1.0_F2°0.5_ _C3°1.0_E3°1.0_C3°1.0_E3°1.0_G2°1.0_C2°1.0_G2°1.0_C2°1.0_ _C3°0.25_ _G3°1.0_E-3°1.0_G3°1.0_E-3°1.0_B-1°1.0_G2°1.0_B-1°1.0_G2°1.0_ _G3°1.0_E-3°1.0_G3°1.0_E-3°1.0_B-1°1.0_G2°1.0_B-1°1.0_G2°1.0_ _D3°1.0_F3°1.0_D3°1.0_F3°1.0_F2°1.0_B-1°1.0_F2°1.0_B-1°1.0_ _D3°1.0_F3°1.0_D3°1.0_F3°1.0_B-1°1.0_F2°1.0_B-1°1.0_F2°1.0_ _B-2°1.0_D3°1.0_B-2°1.0_D3°1.0_ _B2°0.0_ _E-3°1.0_C3°1.0_E-3°1.0_C3°1.0_B-1°1.0_E-2°1.0_B-1°1.0_E-2°1.0_ _C#3°1.0_F3°1.0_C#3°1.0_F3°1.0_A-3°2.0_A-1°1.0_F2°1.0_A-1°1.0_F2°1.0_ _C3°1.0_E-3°1.0_C3°1.0_E-3°1.0_E-2°0.5_ _E-3°1.0_C3°1.0_E-3°1.0_C3°1.0_E-2°1.0_A-1°1.0_E-2°1.0_A-1°1.0_ _A-2°0.25_ _A-3°1.0_C3°1.0_E-3°1.0_A-3°1.0_C3°1.0_E-3°1.0_A-3°1.0_C3°1.0_E-3°1.0_A-1°1.0_E-2°1.0_A-1°1.0_E-2°1.0_ _G3°1.5_C3°1.0_D3°1.0_C3°1.0_D3°1.0_G1°1.0_D2°1.0_G1°1.0_D2°1.0_ _C3°0.25_ _G3°2.0_C3°1.0_F3°1.0_C3°1.0_F3°1.0_G1°1.0_D2°1.0_G1°1.0_D2°1.0_ _B-2°1.0_E-3°1.0_B-2°1.0_E-3°1.0_F1°1.0_C2°1.0_F1°1.0_C2°1.0_ _B-2°1.0_G3°1.0_E-3°1.0_B-2°1.0_G3°1.0_E-3°1.0_B-2°1.0_G3°1.0_E-3°1.0_C2°1.0_F1°1.0_C2°1.0_F1°1.0_ _E-1°1.0_B-1°1.0_E-1°1.0_B-1°1.0_ _G3°1.0_B-2°1.0_E-3°1.0_G3°1.0_B-2°1.0_E-3°1.0_G3°1.0_B-2°1.0_E-3°1.0_E-1°1.0_B-1°1.0_E-1°1.0_B-1°1.0_ _G3°1.0_E-3°1.0_B-2°1.0_G3°1.0_E-3°1.0_B-2°1.0_G3°1.0_E-3°1.0_B-2°1.0_C2°1.0_F1°1.0_C1°1.0_C2°1.0_F1°1.0_C1°1.0_C2°1.0_F1°1.0_C1°1.0_ _C5°0.25_ _C#5°1.0_D5°1.0_C#5°1.0_D5°1.0_E-5°0.5_B-0°0.0_D3°1.0_F2°1.0_D3°1.0_F2°1.0_ _D5°0.25_ _D5°0.5_ _B-4°0.25_ _C5°0.5_ _B-1°0.25_ _E-4°0.5_B-4°2.0_G2°0.5_ _E4°0.0_ _F4°0.25_D3°1.0_F2°1.0_D3°1.0_F2°1.0_ _E4°0.0_ _F4°0.25_F2°1.0_D3°1.0_F2°1.0_D3°1.0_ _B-3°0.25_B-1°0.25_ _C#4°0.0_ _D4°0.0_E-4°0.25_E-2°1.0_C3°1.0_E-2°1.0_C3°1.0_ _B-1°0.25_ _D4°0.25_D2°1.0_B-2°1.0_D2°1.0_B-2°1.0_ _B-3°0.25_B-1°0.25_ _C4°0.25_C3°1.0_E-2°1.0_C3°1.0_E-2°1.0_ _B-1°0.25_ _D4°0.5_D3°1.0_F2°1.0_D3°1.0_F2°1.0_ _B-3°0.5_ _E-4°0.25_G2°1.0_E-3°1.0_G2°1.0_E-3°1.0_ _B-3°0.5_ _E-4°0.25_G2°1.0_E-3°1.0_G2°1.0_E-3°1.0_ _E-4°0.0_B-3°1.0_F4°0.5_C2°2.0_ _E-4°0.25_ _E-4°0.25_ _C4°0.25_ _D4°0.25_ _F3°1.0_G3°1.0_F3°1.0_G3°1.0_ _C4°1.0_F2°1.0_C3°1.0_F2°1.0_C3°1.0_ _B-3°1.0_ _B-3°1.0_F3°1.0_B-3°1.0_F3°1.0_G2°1.0_C2°1.0_G2°1.0_C2°1.0_ _B-3°1.0_G3°1.0_B-3°1.0_G3°1.0_F2°1.0_C3°1.0_F2°1.0_C3°1.0_ _G3°1.0_E-4°1.0_G3°1.0_E-4°1.0_B-1°1.0_F2°1.0_B-1°1.0_F2°1.0_ _F3°1.0_D4°1.0_F3°1.0_D4°1.0_ _D4°1.0_F3°1.0_D4°1.0_F3°1.0_ _B-3°0.25_ _C4°0.5_ _B-3°2.0_A-3°1.0_F3°1.0_A-3°1.0_F3°1.0_B-2°1.0_E-2°1.0_B-2°1.0_E-2°1.0_ _G3°0.25_ _E-3°0.25_ _F3°1.0_A-3°3.0_ _E-2°1.0_C#3°1.0_A1°1.0_E-2°1.0_C#3°1.0_A1°1.0_E-2°1.0_C#3°1.0_A1°1.0_ _E-3°0.25_C3°0.25_ _E-3°0.75_C3°0.25_ _A-2°0.25_ _B-2°0.25_ _C#3°0.25_A-2°0.25_ _B-3°1.0_F#3°1.0_B-3°1.0_F#3°1.0_C#2°1.0_A-2°1.0_C#2°1.0_A-2°1.0_ _F3°1.0_A-3°1.0_F3°1.0_A-3°1.0_ _F3°1.0_A-3°1.0_F3°1.0_A-3°1.0_ _C#3°0.25_ _A-3°1.0_E-3°1.0_A-3°1.0_E-3°1.0_ _B-2°0.25_ _B3°0.0_E-4°2.0_C4°0.5_E-3°1.0_B-2°1.0_E-2°1.0_E-3°1.0_B-2°1.0_E-2°1.0_E-3°1.0_B-2°1.0_E-2°1.0_ _B-3°0.25_ _B-3°0.25_ _E-3°0.25_ _A-3°0.5_ _F#3°0.0_G2°0.25_ _C4°4.0_G3°1.0_C3°0.75_ _C3°0.25_ _G3°0.5_F3°0.25_ _C3°0.25_ _G3°0.5_E3°0.25_ _C3°0.25_ _G3°1.0_B-3°1.0_G3°1.0_B-3°1.0_F2°1.5_ _A3°1.0_F3°1.0_A3°1.0_F3°1.0_ _A3°1.0_F3°1.0_A3°1.0_F3°1.0_ _F3°0.25_ _G3°0.25_ _F3°0.25_A2°0.25_ _F#3°1.0_F#4°1.0_F#3°1.0_F#4°1.0_G4°1.0_D4°1.0_G4°1.0_D4°1.0_G3°0.75_D2°2.0_ _F#4°0.25_ _F#4°0.25_ _D3°0.5_ _D4°0.5_ _F3°0.25_E4°0.5_ _F3°0.25_G3°1.0_A3°1.0_G3°1.0_A3°1.0_ _D4°2.0_B3°0.5_B2°1.0_G2°1.0_B2°1.0_G2°1.0_ _A3°0.25_ _A3°0.5_ _F#2°1.0_D3°1.0_F#2°1.0_D3°1.0_ _G3°0.25_ _A3°0.25_ _D3°0.25_ _G3°0.5_E3°1.0_E2°1.0_E3°1.0_E2°1.0_ _E3°0.5_ _C4°0.25_G3°0.5_C3°1.0_ _B2°0.25_ _D3°0.25_A3°0.5_ _E3°0.25_ _A3°1.0_D3°1.0_A3°1.0_D3°1.0_E1°1.0_ _B2°1.0_G3°1.0_B2°1.0_G3°1.0_ _E1°0.5_ _E3°0.25_ _B2°0.25_G1°0.25_ _D3°1.0_G3°1.0_D3°1.0_G3°1.0_A1°1.0_ _B3°0.25_ _E3°0.25_ _A3°1.0_C#3°1.0_A3°1.0_C#3°1.0_ _G3°1.0_D3°1.0_G3°1.0_D3°1.0_A0°0.25_ _E3°0.25_C#1°0.25_ _C3°0.25_D1°2.25_ _G3°1.0_B-3°1.0_G3°1.0_B-3°1.0_ _F3°1.0_A-3°1.0_F3°1.0_A-3°1.0_ _F3°1.0_A-3°1.0_F3°1.0_A-3°1.0_ _G3°1.0_E-3°1.0_G3°1.0_E-3°1.0_ _F3°1.0_D3°1.0_F3°1.0_D3°1.0_ _E-3°1.0_G3°1.0_E-3°1.0_G3°1.0_ _B-2°0.25_ _B-2°0.25_ _A-2°0.25_ _C3°0.25_ _G2°0.25_ _D2°4.0_ _C3°0.25_ _B-2°0.25_ _C3°0.25_ _A-2°0.25_ _C3°0.5_ _G2°0.25_ _C3°0.25_ _A-2°0.25_ _C3°0.25_ _C3°0.25_ _D2°4.0_ _E-3°0.5_C3°0.25_ _B-2°0.25_ _E-3°0.25_C3°0.25_ _F3°0.25_A-2°0.25_ _E-3°0.5_C3°0.5_ _F3°0.25_G2°0.25_ _E-3°0.25_C3°0.25_ _A-2°0.25_ _F3°0.25_C3°0.25_ _F3°0.25_C3°0.25_ _D2°4.0_ _F3°0.25_C3°0.25_ _E-3°0.25_B-2°0.25_ _F3°0.25_C3°0.25_ _E-3°0.25_A-2°0.25_ _F3°0.75_C3°0.5_ _G2°0.25_ _F3°0.25_C3°0.25_ _E-3°0.25_ _B-2°0.25_ _F3°0.25_ _C3°0.25_ _D4°8.0_D2°4.0_ _B-3°0.0_ _C4°0.5_C3°0.25_ _B-2°0.25_ _A-3°0.25_C3°0.25_ _B-3°0.25_A-2°0.25_ _A-3°0.5_C3°0.5_ _D3°0.25_B-2°0.25_ _F3°0.25_C3°0.25_ _G3°0.25_G2°0.25_ _D3°0.25_ _F3°0.75_C3°0.75_ _D2°4.0_ _D3°0.25_C3°0.25_ _D3°0.25_B-2°0.25_ _F3°0.25_ _C3°0.25_ _G3°0.25_A-2°0.25_ _D3°0.25_ _D3°0.25_C3°0.25_ _F3°0.25_ _D3°0.25_B-2°0.25_ _G3°1.0_G2°0.5_ _C3°0.25_ _B-2°0'"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classicSentence = \"\"\n",
    "for sentence in musicDB[\"Piece\"] :\n",
    "    classicSentence+=sentence+\" \"\n",
    "    \n",
    "classicSentence = classicSentence[:10000]\n",
    "classicSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_input, integer_encoded, token, vocabLength = sentenceEncoding(classicSentence, \"character\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1630"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modele LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import music21\n",
    "from glob import glob\n",
    "import IPython\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences(notes, n_vocab): \n",
    "    sequence_length = 100\n",
    "\n",
    "    # Extract the unique pitches in the list of notes.\n",
    "    pitchnames = sorted(set(item for item in notes))\n",
    "\n",
    "    # Create a dictionary to map pitches to integers\n",
    "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
    "\n",
    "    network_input = []\n",
    "    network_output = []\n",
    "\n",
    "    # create input sequences and the corresponding outputs\n",
    "    for i in range(0, len(notes) - sequence_length):\n",
    "        sequence_in = notes[i: i + sequence_length]\n",
    "        sequence_out = notes[i + sequence_length]\n",
    "        network_input.append([note_to_int[char] for char in sequence_in])\n",
    "        network_output.append(note_to_int[sequence_out])\n",
    "    \n",
    "    n_patterns = len(network_input)\n",
    "    \n",
    "    # reshape the input into a format comatible with LSTM layers \n",
    "    network_input = np.reshape(network_input, (n_patterns, sequence_length, 1))\n",
    "    \n",
    "    # normalize input\n",
    "    network_input = network_input / float(n_vocab)\n",
    "    \n",
    "    # one hot encode the output vectors\n",
    "    network_output = np_utils.to_categorical(network_output)\n",
    "    \n",
    "    return network_input, network_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_network(network_in, n_vocab): \n",
    "    \"\"\"Create the model architecture\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, input_shape=network_in.shape[1:], return_sequences=True))\n",
    "    #model.add(Dropout(0.2))\n",
    "    #model.add(LSTM(128, return_sequences=True))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256))\n",
    "    #model.add(Dropout(0.3))\n",
    "    model.add(Dense(n_vocab))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "    return model\n",
    "\n",
    "def train(model, network_input, network_output, epochs): \n",
    "    \"\"\"\n",
    "    Train the neural network\n",
    "    \"\"\"\n",
    "    # Create checkpoint to save the best model weights.\n",
    "    filepath = 'Models/weights_Bach_Fugue_20ep.hdf5'\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=0, save_best_only=True)\n",
    "    \n",
    "    model.fit(network_input, network_output, epochs=epochs, batch_size=32, callbacks=[checkpoint])\n",
    "    \n",
    "\n",
    "def train_network(ep = 10):\n",
    "    \"\"\"\n",
    "    Get notes\n",
    "    Generates input and output sequences\n",
    "    Creates a model \n",
    "    Trains the model for the given epochs\n",
    "    \"\"\"\n",
    "    \n",
    "    epochs = ep\n",
    "    \n",
    "    notes = token\n",
    "    print('Notes processed')\n",
    "    \n",
    "    n_vocab = vocabLength+1 ## /!\\ Je comprends pas pourquoi il ya besoins d'ajouter 1\n",
    "    print('Vocab generated')\n",
    "    \n",
    "    network_in, network_out = prepare_sequences(notes, n_vocab)\n",
    "    print('Input and Output processed')\n",
    "    \n",
    "    model = create_network(network_in, n_vocab)\n",
    "    print('Model created')\n",
    "    #return model\n",
    "    print('Training in progress')\n",
    "    train(model, network_in, network_out, epochs)\n",
    "    print('Training completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notes processed\n",
      "Vocab generated\n",
      "Input and Output processed\n",
      "Model created\n",
      "Training in progress\n",
      "Epoch 1/40\n",
      "48/48 [==============================] - 5s 106ms/step - loss: 3.9676\n",
      "Epoch 2/40\n",
      "48/48 [==============================] - 6s 119ms/step - loss: 3.6624\n",
      "Epoch 3/40\n",
      "48/48 [==============================] - 7s 155ms/step - loss: 3.5742\n",
      "Epoch 4/40\n",
      "48/48 [==============================] - 9s 184ms/step - loss: 3.4112\n",
      "Epoch 5/40\n",
      "48/48 [==============================] - 9s 185ms/step - loss: 3.2310\n",
      "Epoch 6/40\n",
      "48/48 [==============================] - 6s 129ms/step - loss: 3.0202\n",
      "Epoch 7/40\n",
      "48/48 [==============================] - 6s 131ms/step - loss: 2.8161\n",
      "Epoch 8/40\n",
      "48/48 [==============================] - 8s 168ms/step - loss: 2.5720\n",
      "Epoch 9/40\n",
      "48/48 [==============================] - 6s 133ms/step - loss: 2.2998\n",
      "Epoch 10/40\n",
      "48/48 [==============================] - 7s 148ms/step - loss: 2.0455\n",
      "Epoch 11/40\n",
      "48/48 [==============================] - 9s 195ms/step - loss: 1.8196\n",
      "Epoch 12/40\n",
      "48/48 [==============================] - 8s 156ms/step - loss: 1.6329\n",
      "Epoch 13/40\n",
      "48/48 [==============================] - 7s 143ms/step - loss: 1.5145\n",
      "Epoch 14/40\n",
      "48/48 [==============================] - 6s 124ms/step - loss: 1.3525\n",
      "Epoch 15/40\n",
      "48/48 [==============================] - 6s 123ms/step - loss: 1.2568\n",
      "Epoch 16/40\n",
      "48/48 [==============================] - 9s 194ms/step - loss: 1.1661\n",
      "Epoch 17/40\n",
      "48/48 [==============================] - 5s 114ms/step - loss: 1.0564\n",
      "Epoch 18/40\n",
      "48/48 [==============================] - 8s 170ms/step - loss: 0.9618\n",
      "Epoch 19/40\n",
      "48/48 [==============================] - 8s 163ms/step - loss: 0.9024\n",
      "Epoch 20/40\n",
      "48/48 [==============================] - 6s 122ms/step - loss: 0.7680\n",
      "Epoch 21/40\n",
      "48/48 [==============================] - 6s 125ms/step - loss: 0.6797\n",
      "Epoch 22/40\n",
      "48/48 [==============================] - 5s 114ms/step - loss: 0.5602\n",
      "Epoch 23/40\n",
      "48/48 [==============================] - 6s 122ms/step - loss: 0.4561\n",
      "Epoch 24/40\n",
      "48/48 [==============================] - 6s 120ms/step - loss: 0.3772\n",
      "Epoch 25/40\n",
      "48/48 [==============================] - 6s 123ms/step - loss: 0.3131\n",
      "Epoch 26/40\n",
      "48/48 [==============================] - 6s 124ms/step - loss: 0.2334\n",
      "Epoch 27/40\n",
      "48/48 [==============================] - 6s 124ms/step - loss: 0.1554\n",
      "Epoch 28/40\n",
      "48/48 [==============================] - 6s 120ms/step - loss: 0.0958\n",
      "Epoch 29/40\n",
      "48/48 [==============================] - 6s 117ms/step - loss: 0.0630\n",
      "Epoch 30/40\n",
      "48/48 [==============================] - 6s 127ms/step - loss: 0.0351\n",
      "Epoch 31/40\n",
      "48/48 [==============================] - 7s 148ms/step - loss: 0.0251\n",
      "Epoch 32/40\n",
      "48/48 [==============================] - 7s 145ms/step - loss: 0.0121\n",
      "Epoch 33/40\n",
      "48/48 [==============================] - 6s 128ms/step - loss: 0.0064\n",
      "Epoch 34/40\n",
      "48/48 [==============================] - 6s 123ms/step - loss: 0.0072\n",
      "Epoch 35/40\n",
      "48/48 [==============================] - 6s 127ms/step - loss: 0.0057\n",
      "Epoch 36/40\n",
      "48/48 [==============================] - 6s 116ms/step - loss: 0.0068\n",
      "Epoch 37/40\n",
      "48/48 [==============================] - 6s 126ms/step - loss: 0.0034\n",
      "Epoch 38/40\n",
      "48/48 [==============================] - 5s 111ms/step - loss: 0.0020\n",
      "Epoch 39/40\n",
      "48/48 [==============================] - 5s 109ms/step - loss: 0.0016\n",
      "Epoch 40/40\n",
      "48/48 [==============================] - 6s 119ms/step - loss: 0.0014\n",
      "Training completed\n"
     ]
    }
   ],
   "source": [
    "train_network(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inputSequences(notes, pitchnames, n_vocab):\n",
    "    \"\"\" Prepare the sequences used by the Neural Network \"\"\"\n",
    "    # map between notes and integers and back\n",
    "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
    "\n",
    "    sequence_length = 100\n",
    "    network_input = []\n",
    "    for i in range(0, len(notes) - sequence_length, 1):\n",
    "        sequence_in = notes[i:i + sequence_length]\n",
    "        network_input.append([note_to_int[char] for char in sequence_in])\n",
    "    \n",
    "    network_input = np.reshape(network_input, (len(network_input), 100, 1))\n",
    "    \n",
    "    return (network_input)\n",
    "\n",
    "def generate_notes(model, network_input, pitchnames, n_vocab):\n",
    "    \"\"\" Generate notes from the neural network based on a sequence of notes \"\"\"\n",
    "    # Pick a random integer\n",
    "    start = np.random.randint(0, len(network_input)-1)\n",
    "\n",
    "    int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\n",
    "    \n",
    "    # pick a random sequence from the input as a starting point for the prediction\n",
    "    pattern = list(network_input[start])\n",
    "    prediction_output = []\n",
    "    \n",
    "    print('Generating notes........')\n",
    "\n",
    "    # generate 500 notes\n",
    "    for note_index in tqdm(range(500)):\n",
    "        prediction_input = np.reshape(pattern, (1, len(pattern), 1))\n",
    "        prediction_input = prediction_input / float(n_vocab)\n",
    "        prediction_input = np.asarray(prediction_input).astype(np.float32)\n",
    "\n",
    "        prediction = model.predict(prediction_input, verbose=0)\n",
    "        \n",
    "        # Predicted output is the argmax(P(h|D))\n",
    "        \n",
    "        index = np.argmax(prediction)\n",
    "        \n",
    "        #/!\\ Tester de ne pas prendre argmax mais un random choice  \n",
    "        \n",
    "        # Mapping the predicted interger back to the corresponding note\n",
    "        result = int_to_note[index]\n",
    "        # Storing the predicted output\n",
    "        prediction_output.append(result)\n",
    "\n",
    "        pattern.append(index)\n",
    "        # Next input to the model\n",
    "        pattern = pattern[1:len(pattern)]\n",
    "\n",
    "    print('Notes Generated...')\n",
    "    return prediction_output\n",
    "\n",
    "def generate():\n",
    "    \"\"\" Generate a piano midi file \"\"\"\n",
    "    #load the notes used to train the model\n",
    "    notes = token\n",
    "\n",
    "    # Get all pitch names\n",
    "    pitchnames = sorted(set(item for item in notes))\n",
    "    # Get all pitch names\n",
    "    n_vocab = vocabLength+1\n",
    "    \n",
    "    print('Initiating music generation process.......')\n",
    "    \n",
    "    network_input = get_inputSequences(notes, pitchnames, n_vocab)\n",
    "    normalized_input = network_input / float(n_vocab)\n",
    "    model = create_network(normalized_input, n_vocab)\n",
    "    print('Loading Model weights.....')\n",
    "    model.load_weights('Models/weights_Bach_Fugue_20ep.hdf5')\n",
    "    print('Model Loaded')\n",
    "    prediction_output = generate_notes(model, network_input, pitchnames, n_vocab)\n",
    "    return prediction_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating music generation process.......\n",
      "Loading Model weights.....\n",
      "Model Loaded\n",
      "Generating notes........\n",
      "Notes Generated...\n"
     ]
    }
   ],
   "source": [
    "a = generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=token2midi(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
